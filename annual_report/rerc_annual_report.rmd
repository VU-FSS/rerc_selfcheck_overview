---
title: "Research Ethics Review Committee Anual Report 2021-2022"
date: "`r Sys.Date()`"
author: "Koen Leuveld and Ren√© Bekkers"
output: pdf_document
header-includes:
   - \usepackage{booktabs}
---


```{r, include=FALSE}
library(knitr)
library(kableExtra)

#user defined variables:
start_date <- "2022-06-20"
setwd("C:/Users/kld330/git/rerc_selfcheck_overview")
download_data <- F

##further setup (no changes needed)

#make the start date useable to the sourced scripts
#see: https://stackoverflow.com/questions/14525580/how-to-pass-command-line-arguments-when-calling-source-on-an-r-file-within-ano
commandArgs <- function(...) start_date

#Download data to update CSV files used by RERC_analysis.r
if (download_data) {
   source("rerc_selfcheck_qualtricsdownload.R") 
}

#analyze
source("rerc_selfcheck_analysis.r")
#load additional data
staff_data <- read.csv("annual_report/sep_tabellen.csv")
student_data <- read.csv("annual_report/Studenten_tabellen.csv")

```



# Introduction
This memo reports research ethics review activities at the Faculty of Social Sciences at Vrije Universiteit Amsterdam in the academic year 2021/2022. Since 2016 the RERC has reviewed ethics issues of research by faculty members. The report gives an overview of the composition of the RERC and its activities, including the number self-checks done by faculty members and students and of the applications for full review. 

<!-- Rene: RB attended NETHICS meetings. 
A special meeting of the BMDO/OPCO, with action plan
No letters to the FB -->

# Membership Composition
The RERC consists of five members, one from each department of the Faculty. The RERC is supported by a secretary supplied by the Research Support Office: Koen Leuveld, who is also Data Steward and Privacy Champion. The members, and the dates by which their appointments end, are listed in Table 1. Throughout the academic year (September - June) members meet on a monthly basis to discuss ethics issues and policies within the faculty.

```{r,  echo=FALSE, message = FALSE}
read_csv("Term_limits.csv") %>%
kable(booktabs = T, 
      longtable=F,
      caption="Composition of the RERC",
      position="h") %>%
kable_styling(latex_options = "striped") %>%
add_footnote(
  label = "Peter Versteeg will be replaced by Luisa Schneider, effective 1.9.2022",
  notation = "none"
)
```

# Self-check
Since 2017 the RERC provides an online self-check tool^[The self-check is a Qualtrics survey consisting of 16 questions about potential ethics issues: https://vuletteren.eu.qualtrics.com/jfe/form/SV_6hCj2czIWzboW6V] that researchers and students can use to screen their research plans for ethics issues. The analysis below is based on data entered in the self-check and full application surveys.

Since its inception, the self-check tool has been used more than 1,700 times. Table 2 provides a breakdown of the use of the tool by FSS department.

```{r total_by_year,  echo=FALSE, , message=F}

table_by_period(data=self_check_all_month,
                period = "Year",
                num_periods=6,
                cols="Year") %>%
kable(booktabs = T, 
      longtable=F,
      caption="Number of completed checks per department",
      position="h") %>%
kable_styling(latex_options = "striped") %>%
row_spec(5,hline_after=TRUE)
```

Communication Science (COM) is by far the most active user of the self-check. The tool is less frequently used at Political Science and Public Administration (PSPA) , Sociology (SOC), and Organization Science (ORG). The tool is least frequently used at Social and Cultural Anthropology (SCA).

Since the 2017-2018 academic year, RERC members have encouraged students in the master programs to use the tool. Indeed the data show that the tool is most frequently used by students in master programs. 

Table 3 provides a breakdown of use by researchers alone, excluding students. Furthermore, the table includes each department's research input in FTE, as provided in the SEP tables. We see that in absolute number of checks COM remains the most active, with almost twice the numbers of PSPA, ORG and SOC. SCA  uses the self-check tool least intensively. However, to a large extent, these differences can be attributed to the size of the various deparetments. If we compare the number of checks completed with the number of FTE (the % columns), we see that in 2021 PSPA and COM department members completed on average 2 checks per research FTE. ORG and SCA completed nearly 1 check per research FTE. Sociology lags behind with 0.6 check per research FTE. 

Differences between departments in the numbers of checks per research FTE are likely to be due to the type of data researchers collect and analyze. Ethics review is connected to data collection. Researchers at SOC often use secondary data, for which ethics review has been completed by the team that initially collected the data. Researchers at COM often collect data in experiments, which can be designed and implemented in a shorter period of time than ethnographic data collections. 


```{r staff_by_year_FTE, echo=FALSE, , message=F}
table_by_period(data=self_check_all_month,
                position="Staff",
                period="Year",
                num_periods=4,
                join=staff_data[-3],
                cols="Year",
                dept_total=FALSE,
                prop_col=TRUE) %>%
kable(booktabs = T, 
      longtable=F,
      caption="Number of completed checks excluding students and research time in FTE, by department",
      position="h",
      align = c("l",
                    rep(c("r","r","r"),4)),
      col.names = c("Dept",
                    rep(c("Chcks","FTE","%"),4))) %>%
kable_styling(latex_options = "striped") %>%
row_spec(5,hline_after=TRUE) %>%
add_header_above(header = c(" " = 1,
                          "2018" =3,
                          "2019" =3,
                          "2020" =3,
                          "2021" =3))

```


Table 4 is similar to Table 3, however rather than considering research inputs, it compares checks to research outputs^[The SEP tables include Refereed articles, Non-refereed articles, Books, Book chapters, PhD-theses, Professional publications, Publications general public, and Other research output. The numbers reported here are a sum of these outputs], again obtained from the SEP tables. COM uses the tool most intensively per research output, about half a check per publication in 2021 (54%). This is about twice as intensively as PSPA (33%) and ORG (24%). SOC (14%) and SCA (10%) use the self-check tool least intensively. Again, differences between departments in the type of data collected and analyzed are important reasons for differences in the use of the self-check per publication. Also differences in the types of publication are likely to be important. SCA and ORG publish more non-empirical work than the other departments. They may also base more publications on one data collection effort (for which one self-check is needed).


```{r staff_by_year_pub, echo=FALSE, message = F}

table_by_period(data=self_check_all_month,
                position="Staff",
                period="Year",
                num_periods=4,
                join=staff_data[-4],
                cols="Year",
                dept_total=FALSE,
                prop_col=TRUE) %>%
kable(booktabs = T, 
      longtable=F,
      caption="Number of completed checks excluding students and total research outputs, by department",
      position="h",
      align = c("l",
                    rep(c("r","r","r"),4)),
      col.names = c("Dept",
                    rep(c("Chcks","Pubs","%"),4))) %>%
kable_styling(latex_options = "striped") %>%
row_spec(5,hline_after=TRUE) %>%
add_header_above(header = c(" " =  1,
                          "2018" = 3,
                          "2019" = 3,
                          "2020" = 3,
                          "2021" = 3))

```

Table 5 compares the numbers of students in the programs from each department to the numbers of self-checks done. Overall, the proportion of checks done per student has risen considerably over the four years considered: from 20% to 72%. In some departments, the propotion now exceeds 100%, which is due to students performing multiple self-checks. This is encouraged, as one motivation behind the self-check is to encourage people to change their research design to avoid ethics issues. The adapted research must then be subjected to a new self-check. Interestingly, the proportion of checks among staff does not correlate poistively with the numbers done by students. SCA and SOC have low proportions of checks among staff, but very high proportions among students. The low number of checks at ORG is cause for concern.


```{r students_by_year, echo=FALSE}
table_by_period(data=self_check_all_month,
               position="Student",
               period="Academic_Year",
               num_periods=4,
               join=student_data,
               cols="Academic_Year",
               dept_total=FALSE,
               prop_col=TRUE) %>%
kable(booktabs = T, 
      longtable=F,
      caption="Number of completed checks by students and total number of master students, by department",
      position="h",
      align = c("l",
                    rep(c("r","r","r"),4)),
      col.names = c("Dept",
                    rep(c("Chcks","Stud.","%"),4))) %>%
kable_styling(latex_options = "striped") %>%
row_spec(5,hline_after=TRUE) %>%
add_header_above(header = c(" " = 1,
                          "2018/2019" =3,
                          "2019/2020" =3,
                          "2020/2021" =3,
                          "2021/2022" =3))

```

# Full Review

If the self-check reveals potential ethics issues, these are then discussed with the relevant department's representative in the RERC. If the ethics issues are serious enough, researchers should request a full review of the study. Table 6 shows the total number of applications for full review done through the Qualtrics Form^[The full review qualtrics form can be found here:https://vuass.eu.qualtrics.com/jfe/form/SV_9tBjPqFq6bxv2Sx], compared with the total number of completed self-checks by department. Not all of the applications for full review will be taken into consideration by the committee, since some of them concern applications by Master students, which the RERC does not review. Overall, it is clear from the table that the numbers of full review are low, and that only a fraction of all research projects for which researchers at the faculty completed a self-check necessitated a full review.


```{r full_review_vs_selfcheck, echo=FALSE}
table_by_period(data=full_review,
               period="Year",
               num_periods=4,
               join=self_check_all_year,
               cols="Year",
               dept_total=FALSE,
               prop_col=TRUE) %>%
kable(booktabs = T, 
      longtable=F,
      caption="Number of Full Reviews requested and number of self-checks completed, by department",
      position="h",
      align = c("l",
                rep(c("r","r","r"),4)),
      col.names = c("Dept",
                    rep(c("Rev.","Chks.","%"),4))) %>%
kable_styling(latex_options = "striped") %>%
row_spec(5,hline_after=TRUE) %>%
add_header_above(header = c(" " =  1,
                          "2019" = 3,
                          "2020" = 3,
                          "2021" = 3,
                          "2022" = 3))

```

# Website and outreach activities
The RERC provides all materials researchers need for ethics review through its website^[The RERC website can be found here: https://vu.nl/en/employee/social-sciences-getting-started/research-ethics-review-fss]. In 2021 the committee revised the website to clarify the procedure, particularly for students and PhD candidates, and improved the self-check questionnaire to enable researchers working with secondary data to complete the self-check. The self-check also provides clearer guidance on the next steps researchers should take in order to ensure compliance with relevant policies.

# Policy development and other activities

In 2021-2022 the committee worked to further improve the ethics review procedure. Also the committee worked to increase the knowledge among students and scientific staff about ethics review. 

To promote awareness of ethics review, all PhD students at the Faculty of Social Sciences are required by the Graduate School to complete the course Research Integrity & Responsible Scholarship. The course received a commendation from the Council of Europe as a good practice for research integrity. One of the assignments in the course is that PhD students do the ethics self-check for their dissertation plans, and discuss the result of the check with their supervisors. In this way scientific staff become aware of the ethics review procedures. However, we do not reach faculty members who are not supervising PhD students. Further actions are necessary to enhance the use of the self-check by scientific staff. 

To promote the use of the ethics self-check among students in master programs, members of the committee advocated including a mandatory check in programs taught by their departments. As a result, the self-check has become a mandatory element for thesis projects by master students in Social and Cultural Anthropology, Communication Science, Sociology, and for students in the Research Master Societal Resilience. In a joint BMDO/OPCO meeting, the coordinators and directors of programs taught at the faculty discussed research integrity and ethics review in bachelor and master programs. The director of education proposes to identify research integrity and ethics review as skills that programs teach and evaluate. Obstacles to achieve this goal are the difficulty of data sharing possibilities for students, and the resistance against data management and ethics review among some researchers, primarily those working with ethnographic methods. With support from the Faculty, the RERC chair has conducted interviews to summarize current practices of data sharing, and to identify ways in which researchers can be encouraged to prepare data collection in such a way that their data can be shared in a responsible manner. Together with a research assistant the RERC chair has developed a quiz on research integrity that programs can use to test knowledge on research integrity.

The chair of the committee participated in meetings of the Netherlands network of ethics committees at the Faculties of Social and Behavioral Sciences (NETHICS). We'd like to share two insights from these meetings. The first insight is that most universities have more stringent procedures for ethics review than we do. Many other universities have a "no, unless" policy, requiring all research projects to have clearance from the ethics review committee before data can be collected. Instead of such a requirement we have an optional policy, relying on the individual responsibility of researchers to seek ethics advice when they need it. One result of the optional policy is that the committee does not review all research with ethics issues. The committee does not have indications that researchers at FSS are conducting harmful research. While the tables do indicate that for large proportions of publications by researchers there are no self-checks, to a large extent these publications are not likely to report about data collected at the Faculty. Surveys of FSS publications (Bekkers & Van Pelt, 2016; Doeswijk & Bekkers, 2020; Bekkers & Suijs, 2022) show that large proportions of publications at FSS are not empirical or report about data collected by others, for which ethics review has been completed elsewhere. Thus the committee proposes to keep the current optional procedure, but strive for enhanced usage of the self-check. The second insight is that other universities have streamlined the procedures for data management, privacy checks, and ethics review, which makes it easy for researchers to keep track of their administrative responsibilities. At FSS researchers have to find their way at three different websites and offices. A dashboard identifying whether administrative responsibilities have been met at various stages of the research would be helpful, for researchers and the Research Office. Thus the committee proposes that the University Library develops such a dashboard.


# Looking ahead
For the 2022-2023 academic year, the RERC sees three challenges:

1. Further enhance the use of the self-check by researchers, particularly at departments that are not often using it.
2. Further enhance the use of the self-check by students, particularly in programs that are not often using it.
3. Extend the self-check and full review questionnaire to cover the use of Artificial Intelligence and particularly machine learning algorithms.





# References
Bekkers, R. & Suijs, J. (2022) Data in VU-FSS publications in 2021. 

Bekkers, R. & Van Pelt, B. (2016). Data in VU-FSS publications in 2013 and 2014.

Doeswijk, J. & Bekkers, R. (2020). Data in VU‚ÄêFSS publications in 2018.
